/**
 *
 * Ecmascript 6 module of gary court's amazing work.
 * I just added the export statement and provided the npm configuration.
 *
 * JS Implementation of MurmurHash3 (r136) (as of May 20, 2011)
 *
 * @author <a href="mailto:gary.court@gmail.com">Gary Court</a>
 * @see http://github.com/garycourt/murmurhash-js
 * @author <a href="mailto:aappleby@gmail.com">Austin Appleby</a>
 * @see http://sites.google.com/site/murmurhash/
 *
 * @param {string} key ASCII only
 * @param {number} seed Positive integer only
 * @return {number} 32-bit positive integer hash
 */

 function murmurhash3_32_gc(key, seed) {
  var remainder, bytes, h1, h1b, c1, c2, k1, i;

  remainder = key.length & 3; // key.length % 4
  bytes = key.length - remainder;
  h1 = seed;
  c1 = 0xcc9e2d51;
  c2 = 0x1b873593;
  i = 0;

  while (i < bytes) {
    k1 = key.charCodeAt(i) & 0xff | (key.charCodeAt(++i) & 0xff) << 8 | (key.charCodeAt(++i) & 0xff) << 16 | (key.charCodeAt(++i) & 0xff) << 24;
    ++i;

    k1 = (k1 & 0xffff) * c1 + (((k1 >>> 16) * c1 & 0xffff) << 16) & 0xffffffff;
    k1 = k1 << 15 | k1 >>> 17;
    k1 = (k1 & 0xffff) * c2 + (((k1 >>> 16) * c2 & 0xffff) << 16) & 0xffffffff;

    h1 ^= k1;
    h1 = h1 << 13 | h1 >>> 19;
    h1b = (h1 & 0xffff) * 5 + (((h1 >>> 16) * 5 & 0xffff) << 16) & 0xffffffff;
    h1 = (h1b & 0xffff) + 0x6b64 + (((h1b >>> 16) + 0xe654 & 0xffff) << 16);
  }

  k1 = 0;

  switch (remainder) {
    case 3:
      k1 ^= (key.charCodeAt(i + 2) & 0xff) << 16;
    case 2:
      k1 ^= (key.charCodeAt(i + 1) & 0xff) << 8;
    case 1:
      k1 ^= key.charCodeAt(i) & 0xff;

      k1 = (k1 & 0xffff) * c1 + (((k1 >>> 16) * c1 & 0xffff) << 16) & 0xffffffff;
      k1 = k1 << 15 | k1 >>> 17;
      k1 = (k1 & 0xffff) * c2 + (((k1 >>> 16) * c2 & 0xffff) << 16) & 0xffffffff;
      h1 ^= k1;
  }

  h1 ^= key.length;

  h1 ^= h1 >>> 16;
  h1 = (h1 & 0xffff) * 0x85ebca6b + (((h1 >>> 16) * 0x85ebca6b & 0xffff) << 16) & 0xffffffff;
  h1 ^= h1 >>> 13;
  h1 = (h1 & 0xffff) * 0xc2b2ae35 + (((h1 >>> 16) * 0xc2b2ae35 & 0xffff) << 16) & 0xffffffff;
  h1 ^= h1 >>> 16;

  return h1 >>> 0;
}

const timestampNumbers = (millis, counter) => !isNaN(millis) && !isNaN(counter);
const invalidTimestampString = (timestamp) => typeof timestamp !== "string";
let config = { maxDrift: 60000 };
class Timestamp {
  constructor(millis, counter, node, config = { maxDrift: 60000 }) {
    this._state = {
      millis,
      counter,
      node,
    };
    Object.keys(config).map((key) => (this._state[key] = config[key]));
  }

  valueOf() {
    return this.toString();
  }

  toString() {
    return [
      new Date(this.millis()).toISOString(),
      ("0000" + this.counter().toString(16).toUpperCase()).slice(-4),
      ("0000000000000000" + this.node()).slice(-16),
    ].join("-");
  }
  static maxDrift() {
    return config.maxDrift;
  }

  millis() {
    return this._state.millis;
  }

  counter() {
    return this._state.counter;
  }

  node() {
    return this._state.node;
  }

  hash() {
    return murmurhash3_32_gc(this.toString());
  }

  static send(clock) {
    // Retrieve the local wall time
    const phys = Date.now();

    // Unpack the clock.timestamp logical time and counter
    const tOld = clock.timestamp.millis();
    const cOld = clock.timestamp.counter();

    // Calculate the next logical time and the counter.
    /**
     * - Ensure that the logical time never goes backward.
     * - Increment the counter IF the physical time does not advance
     */
    const tNew = Math.max(tOld, phys);
    const cNew = tOld === tNew ? cOld + 1 : 0;

    /** Check the result for drift and counter overflow */
    if (tNew - phys > this.maxDrift()) {
      throw new Timestamp.ClockDriftError(tNew, phys, this.maxDrift());
    }

    if (cNew > 65535) {
      throw new Timestamp.OverflowError();
    }

    // Repack the logical time/counter.
    clock.timestamp.setMillis(tNew);
    clock.timestamp.setCounter(cNew);

    return new Timestamp(
      clock.timestamp.millis(),
      clock.timestamp.counter(),
      clock.timestamp.node()
    );
  }

  static receive(clock, msg) {
    const phys = Date.now();

    // Unpack the message wall time/counter
    const tMsg = msg.millis();
    const cMsg = msg.counter();

    // Assert the node id and remote clock drift
    if (msg.node() === clock.timestamp.node()) {
      throw new Timestamp.DuplicateNodeError(clock.timestamp.node());
    }
    if (tMsg - phys > this.maxDrift) {
      throw new Timestamp.ClockDriftError();
    }

    // Unpack the clock.timestamp logical time and counter
    const tOld = clock.timestamp.millis();
    const cOld = clock.timestamp.counter();

    // Calculate the next logical time and counter.
    // Ensure that the logical time never goes backward;
    // * if all logical clocks are equal, increment the max counter,
    // * if max = old > message, increment local counter,
    // * if max = message > old, increment message counter,
    // * otherwise, clocks are monotonic, reset counter
    const tNew = Math.max(Math.max(tOld, phys), tMsg);
    const cNew =
      tNew === tOld && tNew === tMsg
        ? Math.max(cOld, cMsg) + 1
        : tNew === tOld
        ? cOld + 1
        : tNew === tMsg
        ? cMsg + 1
        : 0;

    // Check the result for drift and counter overflow
    if (tNew - phys > this.maxDrift) {
      throw new Timestamp.ClockDriftError();
    }
    if (cNew > 65535) {
      throw new Timestamp.OverflowError();
    }

    // Repack the logical time/counter
    clock.timestamp.setMillis(tNew);
    clock.timestamp.setCounter(cNew);

    return new Timestamp(
      clock.timestamp.millis(),
      clock.timestamp.counter(),
      clock.timestamp.node()
    );
  }
  /**
   * Converts a fixed-length string timestamp to the structured value.
   * @param {*} timestamp
   */

  /**
   *
   * @param {*} isoString
   */
  since(isoString) {
    return isoString + "-0000-0000000000000000";
  }

  static makeClock(timestamp, merkle = {}) {
    return { timestamp: MutableTimestamp.from(timestamp), merkle };
  }
  static DuplicateNodeError() {
    return class extends Error {
      constructor(node) {
        super();
        this.type = "DuplicateNodeError";
        this.message = "duplicate node identifier " + node;
      }
    };
  }
  static ClockDriftError() {
    return class extends Error {
      constructor(...args) {
        super();
        this.type = "ClockDriftError";
        this.message = ["maximum clock drift exceeded"].concat(args).join(" ");
      }
    };
  }

  static OverflowError() {
    return class extends Error {
      constructor() {
        super();
        this.type = "OverflowError";
        this.message = "timestamp counter overflow";
      }
    };
  }

  static parse(timestamp) {
    if (invalidTimestampString(timestamp)) {
      return null;
    }

    const parts = timestamp.split("-");
    if (parts && parts.length === 5) {
      const millis = Date.parse(parts.slice(0, 3).join("-")).valueOf();
      const counter = parseInt(parts[3], 16);
      const node = parts[4];
      if (timestampNumbers(millis, counter)) {
        return new Timestamp(millis, counter, node);
      }
    }
  }

  static from(timestamp) {
    return new MutableTimestamp(
      timestamp.millis(),
      timestamp.counter(),
      timestamp.node()
    );
  }

  static serializeClock(clock) {
    return JSON.stringify({
      timestamp: clock.timestamp.toString(),
      merkle: clock.merkle,
    });
  }

  static from(timestamp) {
    return new MutableTimestamp(
      timestamp.millis(),
      timestamp.counter(),
      timestamp.node()
    );
  }
  static deserializeClock(clock) {
    const data = JSON.parse(clock);
    return {
      timestamp: Timestamp.from(Timestamp.parse(data.timestamp)),
      merkle: data.merkle,
    };
  }
}

class MutableTimestamp extends Timestamp {
  setMillis(n) {
    this._state.millis = n;
  }

  setCounter(n) {
    this._state.counter = n;
  }

  setNode(n) {
    this._state.node = n;
  }
}

const getKeys = (trie) => {
  return Object.keys(trie).filter((x) => x !== "hash");
};

const keyToTimestamp = (key) => {
  // 16 is the length of the base 3 value of the current time in
  // minutes. Ensure it's padded to create the full value
  let fullKey = key + "0".repeat(16 - key.length);

  // Parse the base 3 representation
  return parseInt(fullKey, 3) * 1000 * 60;
};

const insertKey = (trie, key, hash) => {
  if (key.length === 0) {
    return trie;
  }
  const c = key[0];
  const n = trie[c] || {};

  return Object.assign({}, trie, {
    [c]: Object.assign({}, n, insertKey(n, key.slice(1), hash), {
      hash: n.hash ^ hash,
    }),
  });
};

/**
 * Inserts base3 key into merkle table
 * @param trie
 * @param timestamp
 */
const insert$1 = (trie, timestamp) => {
  let hash = timestamp.hash();
  let key = Number((timestamp.millis() / 1000 / 60) | 0).toString(3);

  trie = Object.assign({}, trie, { hash: trie.hash ^ hash });
  return insertKey(trie, key, hash);
};

const build = (timestamps) => {
  let trie = {};
  let tTrie = {};
  for (let timestamp of timestamps) {
    tTrie = insert$1(trie, timestamp);
  }

  return tTrie;
};

const diff = (trie1, trie2) => {
  if (trie1.hash === trie2.hash) {
    return null;
  }

  let node1 = trie1;
  let node2 = trie2;
  let k = "";

  while (1) {
    let keySet = new Set([...getKeys(node1), ...getKeys(node2)]);
    let keys = [...keySet.values()];
    keys.sort();
    let diffKey = keys.find((key) => {
      let next1 = node1[key] || {};
      let next2 = node2[key] || {};
      return next1.hash !== next2.hash;
    });

    if (!diffKey) {
      return keyToTimestamp(k);
    }

    k += diffKey;
    node1 = node1[diffKey] || {};
    node2 = node2[diffKey] || {};
  }
};

const prune = (trie, n = 2) => {
  // Do nothing if empty
  if (!trie.hash) {
    return trie;
  }

  let keys = getKeys(trie);
  keys.sort();

  let next = { hash: trie.hash };
  keys = keys.slice(-n).map((k) => (next[k] = prune(trie[k], n)));

  return next;
};

const debug = (trie, k = "", indent = 0) => {
  const str =
    " ".repeat(indent) +
    (k !== "" ? `k: ${k} ` : "") +
    `hash: ${trie.hash || "(empty)"}\n`;

  return (
    str +
    getKeys(trie)
      .map((key) => {
        return debug(trie[key], key, indent + 2);
      })
      .join("")
  );
};

var merkle = /*#__PURE__*/Object.freeze({
  __proto__: null,
  getKeys: getKeys,
  keyToTimestamp: keyToTimestamp,
  insert: insert$1,
  build: build,
  diff: diff,
  prune: prune,
  debug: debug
});

const writable = (initial_value = 0) => {
  let value = initial_value; // content of the store
  let subs = []; // subscriber's handlers

  const subscribe = (handler) => {
    subs = [...subs, handler]; // add handler to the array of subscribers
    handler(value); // call handler with current value
    return () => (subs = subs.filter((sub) => sub !== handler)); // return unsubscribe function
  };

  const set = (new_value) => {
    if (JSON.stringify(value) === JSON.stringify(new_value)) {
      console.error("Passed Same Data Through", new_value, value);
      return;
    } // same value, exit
    console.log('Updating Store', new_value, value);
    value = new_value; // update value
    subs.forEach((sub) => sub(value)); // update subscribers
  };

  const update = (update_fn) => {
    set(update_fn(value));
  }; // update function

  return { subscribe, set, update }; // store contract
};

function debounce(func, timeout = 300) {
  let timer;
  return (...args) => {
    clearTimeout(timer);
    timer = setTimeout(() => {
      func.apply(this, args);
    }, timeout);
  };
}
const objIsEmpty = (obj) => {
  if (!obj) {
    return null;
  }
  if (Object.keys(obj).length > 0) {
    return false;
  }
  return true;
};
const v4 = () => {
  let s4 = () => {
    return Math.floor((1 + Math.random()) * 0x10000)
      .toString(16)
      .substring(1);
  };
  //return id of format 'aaaaaaaa'-'aaaa'-'aaaa'-'aaaa'-'aaaaaaaaaaaa'
  return (
    s4() +
    s4() +
    "-" +
    s4() +
    "-" +
    s4() +
    "-" +
    s4() +
    "-" +
    s4() +
    s4() +
    s4()
  )
    .replace(/-/g, "")
    .slice(-16);
};

const makeClientId = (wDashes = false) => {
  return v4().replace(/-/g, "").slice(-16);
};

const InitEnvironmentState = () => {
  const { set, subscribe, update } = writable({});


  return {
    set,
    subscribe,
    update,
  };
};

const EnvironmentState = InitEnvironmentState();

// The reason for this strange abstraction is because we can't rely on
// nested worker support (Safari doesn't support it). We need to proxy
// creating a child worker through the main thread, and this requires
// a bit of glue code. We don't want to duplicate this code in each
// backend that needs it, so this module abstracts it out. It has to
// have a strange shape because we don't want to eagerly bundle the
// backend code, so users of this code need to pass an `() =>
// import('worker.js')` expression to get the worker module to run.

function isWorker() {
  return (
    typeof WorkerGlobalScope !== 'undefined' &&
    self instanceof WorkerGlobalScope
  );
}

function makeStartWorkerFromMain(getModule) {
  return (argBuffer, resultBuffer, parentWorker) => {
    if (isWorker()) {
      throw new Error(
        '`startWorkerFromMain` should only be called from the main thread'
      );
    }

    if (typeof Worker === 'undefined') {
      // We're on the main thread? Weird: it doesn't have workers
      throw new Error(
        'Web workers not available. sqlite3 requires web workers to work.'
      );
    }

    getModule().then(({ default: BackendWorker }) => {
      let worker = new BackendWorker();

      worker.postMessage({ type: 'init', buffers: [argBuffer, resultBuffer] });

      worker.addEventListener('message', msg => {
        // Forward any messages to the worker that's supposed
        // to be the parent
        parentWorker.postMessage(msg.data);
      });
    });
  };
}

function makeInitBackend(spawnEventName, getModule) {
  const startWorkerFromMain = makeStartWorkerFromMain(getModule);

  return worker => {
    worker.addEventListener('message', e => {
      switch (e.data.type) {
        case spawnEventName:
          startWorkerFromMain(e.data.argBuffer, e.data.resultBuffer, worker);
          break;
      }
    });
  };
}

// Use the generic main thread module to create our indexeddb worker
// proxy
const initBackend = makeInitBackend('__absurd:spawn-idb-worker', () =>
  Promise.resolve().then(function () { return indexeddbMainThreadWorkerE59fee74; })
);

const InitDataStore = () => {
  const { set, update, subscribe } = writable({});

  const methods = {
    addDatastores(schema) {
      if (Array.isArray(schema)) {
        const storeNames = Object.keys(schema).reduce(
          (acc, key) => [...acc, key],
          []
        );
        console.log("Creating Stores", storeNames);
        storeNames.map((st) =>
          update((sch) => {
            sch[st] = [];
            return sch;
          })
        );
      } else {
        update((sch) => {
          sch[schema] = [];
          return sch;
        });
      }
    },
    addRecord(store, record) {
      return update((sch) => {
        return Object.assign(
          { ...sch },
          {
            [store]: [...sch[store], record],
          }
        );
      });
    },
  };

  return {
    ...methods,
    set,
    update,
    subscribe,
  };
};

const datastore = InitDataStore();

const getWorker = () => {
  if (!window.worker) {
    const newWorker = new Worker(new URL("../lib/sync.js", import.meta.url));
    initBackend(newWorker);
    window.worker = newWorker;
  }
};

const bootstrap = (sch) => {
  let schema = sch;
  Object.keys(sch).forEach((s) => {
    datastore.addDatastores(s);
  });
  getWorker();
  return new Promise((res, err) => {
    window.worker.postMessage({
      type: "ui-invoke",
      name: "init",
      arguments: schema,
    });

    return window.worker.addEventListener("message", function (e) {
      if (e.data.type === "INITIALIZED_APP") {
        return res();
      }
    });
  });
};

class EVENTS {
  static APPLY = "APPLY";

  static RUN_INSERT_MESSAGES = "RUN_INSERT_MESSAGES";
  static INSERT_MESSAGE = "INSERT_MESSAGE";

  static RUN_APPLY = "RUN_APPLY";
  static APPLIED = "APPLIED";

  static START_SYNC = "START_SYNC";
  static STOP_SYNC = "STOP_SYNC";
  static SELECT_GROUP = "SELECT_GROUP";
  static ADD_GROUP = "ADD_GROUP";
  static ADD_SCHEMA = "ADD_SCHEMA";
  static INIT = "INIT";
  static SEND_MESSAGE = "SEND_MESSAGE";

  static GET_MERKLE = "GET_MERKLE";
  static MERKLE = "MERKLE";

  static ADD_ENCRYPTION_URL = "ADD_ENCRYPTION_URL";

  static GET_MOST_RECENT_LOCAL_MESSAGES = "GET_MOST_RECENT_LOCAL_MESSAGES";
  static MOST_RECENT_LOCAL_MESSAGES = "MOST_RECENT_LOCAL_MESSAGES";
}

let _clock = null;
let environment$1;
const unsubscribes$1 = [];

unsubscribes$1.push(
  EnvironmentState.subscribe((e) => {
    environment$1 = e;
    if (!objIsEmpty && e.group_id) ;
  })
);

function setClock(clock) {
  localStorage.setItem("clock", JSON.stringify(clock));
  _clock = clock;
}

function getClock() {
  if (!_clock) {
    var cachedClock = localStorage.getItem("clock");
    if (cachedClock) {
      const clock = JSON.parse(cachedClock);
      setClock(_clock);
      return clock;
    }
  }
  return _clock;
}

async function makeClock(timestamp) {
  getWorker();
  window.worker.postMessage({
    type: EVENTS.GET_MERKLE,
    group_id: environment$1.group_id,
  });
  return new Promise((res, rej) => {
    window.worker.addEventListener("message", (e) => {
      if (e.data.type === EVENTS.MERKLE) {
        console.log("MERKLE:", e.data.results);
        return res({
          timestamp: MutableTimestamp.from(timestamp),
          merkle: e.data.results,
        });
      }
    });
  });
}

const initMessageState = () => {
  const { set, subscribe, update } = writable([]);
  const methods = {
    add(message) {
      update((messages) => [...messages, message]);
    },
    remove(message) {
      update((messages) =>
        messages.filter(
          (m) =>
            m.row !== message.row &&
            m.column !== message.column &&
            message.dataset !== m.dataset
        )
      );
    },
  };

  return {
    ...methods,
    set,
    subscribe,
    update,
  };
};

const MessageState = initMessageState();

class MeadoCrypto {
  constructor() {}
  _key;
  _url;
  get key() {
    return this._key;
  }

  async setKey(options) {
    this._key = await this.makeKey(options);
  }

  async encrypt(content) {
    return await window.crypto.subtle.encrypt(
      { name: "AES-GCM", iv: new Uint8Array(12) /* don't reuse key! */ },
      this.key,
      new TextEncoder().encode(JSON.stringify(content))
    );
  }

  async setURL(url){
    const objectKey = (await window.crypto.subtle.exportKey("jwk", this.key)).k;
    this._url = url + "#key=" + objectKey;
  }

  async makeKey(
    options = {
      algorith: { name: "AES-GCM", length: 128 },
      exportable: true,
      keyUsage: ["encrypt", "decrypt"],
    }
  ) {
    return await window.crypto.subtle.generateKey(
      options.algorith,
      options.exportable,
      options.keyUsage
    );
  }
}

let environment = {};
let unsubscribes = [];
let meadoCrypto;
unsubscribes.push(
  EnvironmentState.subscribe(async (e) => {
    environment = e;
    if (e.encryption) {
      meadoCrypto = new MeadoCrypto();
      await meadoCrypto.setKey();
      await meadoCrypto.setURL(environment.sync_url);
      await meadoCrypto.encrypt("stuff");
      console.log("dingo");
    }
  }),
  MessageState.subscribe((m) => (m))
);

const handleApplyMessage = async (messages) => {
  return new Promise((res, rej) => {
    getWorker();
    window.worker.postMessage({ type: EVENTS.RUN_APPLY, messages: messages });
    window.worker.addEventListener("message", (e) => {
      if (e.data.type === EVENTS.APPLIED) {
        res(true);
      }
    });
  });
};

function serializeValue(value) {
  if (value === null) {
    return "0:";
  } else if (typeof value === "number") {
    return "N:" + value;
  } else if (typeof value === "string") {
    return "S:" + value;
  }

  throw new Error("Unserializable value type: " + JSON.stringify(value));
}

function deserializeValue(value) {
  const type = value[0];
  switch (type) {
    case "0":
      return null;
    case "N":
      return parseFloat(value.slice(2));
    case "S":
      return value.slice(2);
  }

  throw new Error("Invalid type key for value: " + value);
}

// 121
/**
 * Apply the data operation contained in a message to our local data store
 * (i.e., set a new property value for a secified dataset/table/row/column).
 */
const apply = async (messages) => {
  getWorker();
  if (!window.worker) {
    throw new Error(`Error: No Worker`);
  }

  await handleApplyMessage(messages);
  return true;
};

async function post(data) {
  let res = await fetch(`${environment.sync_url}/sync`, {
    method: "POST",
    mode: "cors",
    credentials: "same-origin",
    redirect: "follow",
    referrerPolicy: "no-referrer",
    body: JSON.stringify(data),
    headers: {
      "Access-Control-Allow-Origin": "*",
      "Content-Type": "application/json",
      authorization: environment.user_id,
    },
  });
  res = await res.json();

  if (res.status !== "ok") {
    throw new Error("API error: " + res.reason);
  }
  return res.data;
}

function getMatchingLocalMessage(message) {
  getWorker();
  return new Promise((res, rej) => {
    window.worker.postMessage({
      type: EVENTS.GET_MOST_RECENT_LOCAL_MESSAGES,
      message,
    });
    window.worker.addEventListener("message", (e) => {
      if (e.data.type === EVENTS.MOST_RECENT_LOCAL_MESSAGES) {
        return res(e.data.results);
      }
    });
  });
}

async function mapIncomingToLocalMessagesForField(incomingMessages) {
  const incomingFieldMsgToLocalFieldMsgMap = await incomingMessages.reduce(
    async (acc, curr) => {
      const map = await acc;

      // Attempt to find the most recent local message for the same field as the
      // current incoming message (note that find() can return `undefined` if no
      // match is found).
      const mostRecentLocalMsg = await getMatchingLocalMessage(curr);
      // Note that the incoming message OBJECT is being used as a key here
      // (something you couldn't do if an Object were used insteaad of a Map)
      map.set(curr, mostRecentLocalMsg);
      return map;
    },
    Promise.resolve(new Map())
  );

  return incomingFieldMsgToLocalFieldMsgMap;
}

async function handleInsertMessages(message) {
  return new Promise((res, rej) => {
    window.worker.postMessage({
      type: EVENTS.RUN_INSERT_MESSAGES,
      message,
      group_id: environment.group_id,
    });
    window.worker.addEventListener("message", (e) => {
      if (e.data.type === EVENTS.INSERT_MESSAGE) {
        return res(true);
      }
    });
  });
}

let _apply;
function registerApply(callback) {
  _apply = callback;
}
async function applyMessages(incomingMessages) {
  let incomingToLocalMsgsForField = await mapIncomingToLocalMessagesForField(
    incomingMessages
  );
  let clock = getClock();
  let messages = [];
  let appliedMessages = [];
  // Look at each incoming message. If it's new to us (i.e., we don't have it in
  // our local store), or is newer than the message we have for the same field
  // (i.e., dataset + row + column), then apply it to our local data store and
  // insert it into our local collection of messages and merkle tree (which is
  // basically a specialized index of those messages).
  await incomingMessages.reduce(async (acc, incomingMsgForField) => {
    const prevAcc = await acc;
    // `incomingToLocalMsgsForField` is a Map instance, which means objects
    // can be used as keys. If this is the first time we've encountered the
    // message, then we won't have a _local_ version in the Map and `.get()`
    // will return `undefined`.
    let mostRecentLocalMsgForField =
      incomingToLocalMsgsForField.get(incomingMsgForField);

    // If there is no corresponding local message (i.e., this is a "new" /
    // unknown incoming message), OR the incoming message is "newer" than the
    // one we have, apply the incoming message to our local data store.
    //
    // Note that althought `.timestamp` references an object (i.e., an instance
    // of Timestamp), the JS engine is going to implicitly call the instance's
    // `.valueOf()` method when doing these comparisons. The Timestamp class has
    // a custom implementation of valueOf() that retuns a string. So, in effect,
    // comparing timestamps below is comparing the toString() value of the
    // Timestamp objects.

    if (
      !mostRecentLocalMsgForField ||
      mostRecentLocalMsgForField.timestamp < incomingMsgForField.timestamp
    ) {
      // `apply()` means that we're going to actually update our local data
      // store with the operation contained in the message.
      appliedMessages = [...appliedMessages, incomingMsgForField];
    }

    // If this is a new message that we don't have locally (i.e., we didn't find
    // a corresponding local message for the same dataset/row/column OR we did
    // but it has a different timestamp than ours), we need to add it to our
    // axfrray of local messages and update the merkle tree.
    if (
      !mostRecentLocalMsgForField ||
      mostRecentLocalMsgForField.timestamp !== incomingMsgForField.timestamp
    ) {
      clock.merkle = insert$1(
        clock.merkle,
        Timestamp.parse(incomingMsgForField.timestamp)
      );

      // Add the message to our collection...
      // _messages.push(incomingMsgForField);
      messages.push(incomingMsgForField);
    }
    return prevAcc;
  }, Promise.resolve());
  await apply(appliedMessages);
  await handleInsertMessages(messages);
  return _apply && _apply();
}
async function receiveMessages(messages = []) {
  messages.forEach((msg) => {
    Timestamp.receive(getClock(), Timestamp.parse(msg.timestamp));
  });
  await applyMessages(messages);
}

async function getSortedMessages() {
  return new Promise((res, rej) => {
    window.worker.postMessage({ type: "db-sorted-messages" });
    window.worker.addEventListener("message", (e) => {
      if (e.data.type === "sorted-messages") {
        const messages = e.data.sortedMessages;
        res(messages);
      }
    });
  });
}

async function sync(initialMessages = [], since = null) {
  console.log("Running Sync ", since);
  if (environment.syncDisabled) {
    return;
  }

  if (since) {
    let timestamp = new Timestamp(since, 0, "0").toString();
    let sortedMessages = await getSortedMessages();
    sortedMessages.filter((msg) => msg.timestamp >= timestamp);
  }

  let result;
  try {
    result = await post({
      group_id: environment.group_id,
      client_id: getClock().timestamp.node(),
      messages: initialMessages.map((m) => ({
        ...m,
        value: deserializeValue(m.value),
      })),
      merkle: getClock().merkle,
    });
  } catch (e) {
    throw new Error(`network-failure: ${e}`);
  }

  if (result.messages.length > 0) {
    await receiveMessages(
      result.messages.map((m) => ({ ...m, value: serializeValue(m.value) }))
    );
  }

  let diffTime = diff(result.merkle, getClock().merkle);

  if (diffTime) {
    if (since && since === diffTime) {
      throw new Error(
        "A bug happened while syncing and the client " +
          "was unable to get in sync with the server. " +
          "This is an internal error that shouldn't happen"
      );
    }
    return sync([], diffTime);
  }
}

const buildSchema = (data) => {
  return Object.keys(data).reduce((acc, curr) => {
    const tableNames = Object.keys(data[curr]);
    const tableValues = Object.values(data[curr]);
    const statement = tableNames.reduce((acc, scurr, ix) => {
      if (ix === 0) {
        acc = `CREATE TABLE IF NOT EXISTS ${curr} (tombstone integer,`;
      }

      if (ix === tableNames.length - 1) {
        acc = `${acc} ${scurr} ${tableValues[ix]})`;
      } else {
        acc = `${acc} ${scurr} ${tableValues[ix]},`;
      }

      return acc;
    }, "");
    acc[curr] = statement;
    return acc;
  }, {});
};

const update = async (table, row) => {
  let fields = Object.keys(row).filter((r) => r !== "id");
  sendMessages(
    fields.map((f) => ({
      dataset: table,
      row: row.id,
      column: f,
      value: serializeValue(row[f]),
      // Note that every message we create/send gets its own, globally-unique
      // timestamp. In effect, there is a 1-1 relationship between the time-
      // stamp and this specific message.
      timestamp: Timestamp.send(getClock()).toString(),
    }))
  );
};

const insert = async (table, row) => {
  const id = makeClientId(true);
  const fields = Object.keys(row);

  const messages = [
    ...fields.map((k) => {
      return {
        id: makeClientId(),
        dataset: table,
        row: row.id || id,
        column: k,
        value: serializeValue(row[k]),
        timestamp: Timestamp.send(getClock()).toString(),
      };
    }),
    {
      id: makeClientId(),
      dataset: table,
      row: row.id || id,
      column: "tombstone",
      value: serializeValue(0),
      timestamp: Timestamp.send(getClock()).toString(),
    },
  ];

  sendMessages(messages);

  return id;
};

const _delete = (table, id) => {
  sendMessages([
    {
      dataset: table,
      row: id,
      value: serializeValue(1),
      column: "tombstone",
      timestamp: Timestamp.send(getClock()).toString(),
    },
  ]);
};

const sendMessages = (messages) => {
  applyMessages(messages);
  sync(messages);
};

const initMessageBus = () => {
  const { set, update, subscribe } = writable({});
  const methods = {
    publish(message) {
      set(message);
    },
  };
  return {
    set,
    update,
    subscribe,
    ...methods,
  };
};

const messageBus = initMessageBus();

let LOCK_TYPES = {
  NONE: 0,
  SHARED: 1,
  RESERVED: 2,
  PENDING: 3,
  EXCLUSIVE: 4
};

function getPageSize(bufferView) {
  // See 1.3.2 on https://www.sqlite.org/fileformat.html The page size
  // is stored as a 2 byte integer at the 16th byte. It's stored as
  // big-endian so the first byte is the larger one. Combine it into a
  // single integer.
  let int1 = bufferView[16];
  let int2 = bufferView[17];
  return (int1 << 8) + int2;
}

function isSafeToWrite(localData, diskData) {
  if (localData != null && diskData != null) {
    let localView = new Uint8Array(localData);
    let diskView = new Uint8Array(diskData);

    // See
    // https://github.com/sqlite/sqlite/blob/master/src/pager.c#L93-L96
    // (might be documented somewhere? I didn't see it this clearly in
    // the docs). At least one of these bytes change when sqlite3 writes
    // data. We can check this against our in-memory data to see if it's
    // safe to write (if something changes underneath us, it's not)
    for (let i = 24; i < 40; i++) {
      if (localView[i] !== diskView[i]) {
        return false;
      }
    }
    return true;
  }

  // One of them is null, so it's only safe if to write if both are
  // null, otherwise they are different
  return localData == null && diskData == null;
}

function range(start, end, step) {
  let r = [];
  for (let i = start; i <= end; i += step) {
    r.push(i);
  }
  return r;
}

function getBoundaryIndexes(blockSize, start, end) {
  let startC = start - (start % blockSize);
  let endC = end - 1 - ((end - 1) % blockSize);

  return range(startC, endC, blockSize);
}

function readChunks(chunks, start, end) {
  let buffer = new ArrayBuffer(end - start);
  let bufferView = new Uint8Array(buffer);
  for (let i = 0; i < chunks.length; i++) {
    let chunk = chunks[i];

    // TODO: jest has a bug where we can't do `instanceof ArrayBuffer`
    if (chunk.data.constructor.name !== 'ArrayBuffer') {
      throw new Error('Chunk data is not an ArrayBuffer');
    }

    let cstart = 0;
    let cend = chunk.data.byteLength;

    if (start > chunk.pos) {
      cstart = start - chunk.pos;
    }
    if (end < chunk.pos + chunk.data.byteLength) {
      cend = end - chunk.pos;
    }

    if (cstart > chunk.data.byteLength || cend < 0) {
      continue;
    }

    let len = cend - cstart;

    bufferView.set(
      new Uint8Array(chunk.data, cstart, len),
      chunk.pos - start + cstart
    );
  }

  return buffer;
}

function writeChunks(bufferView, blockSize, start, end) {
  let indexes = getBoundaryIndexes(blockSize, start, end);
  let cursor = 0;

  return indexes
    .map(index => {
      let cstart = 0;
      let cend = blockSize;
      if (start > index && start < index + blockSize) {
        cstart = start - index;
      }
      if (end > index && end < index + blockSize) {
        cend = end - index;
      }

      let len = cend - cstart;
      let chunkBuffer = new ArrayBuffer(blockSize);

      if (start > index + blockSize || end <= index) {
        return null;
      }

      let off = bufferView.byteOffset + cursor;

      let available = bufferView.buffer.byteLength - off;
      if (available <= 0) {
        return null;
      }

      let readLength = Math.min(len, available);

      new Uint8Array(chunkBuffer).set(
        new Uint8Array(bufferView.buffer, off, readLength),
        cstart
      );
      cursor += readLength;

      return {
        pos: index,
        data: chunkBuffer,
        offset: cstart,
        length: readLength
      };
    })
    .filter(Boolean);
}

class File {
  constructor(filename, ops, meta = null) {
    this.filename = filename;
    this.buffer = new Map();
    this.ops = ops;
    this.meta = meta;
    this._metaDirty = false;
    this.writeLock = false;
    this.openHandles = 0;
  }

  bufferChunks(chunks) {
    for (let i = 0; i < chunks.length; i++) {
      let chunk = chunks[i];
      this.buffer.set(chunk.pos, chunk);
    }
  }

  open() {
    this.openHandles++;

    // Don't open the file again if it's already open
    if (this.openHandles === 1) {
      this.ops.open();
      let meta = this.ops.readMeta();

      // It's possible that `setattr` has already been called if opening
      // the file in a mode that truncates it to 0
      if (this.meta == null) {
        if (meta == null) {
          // New file

          meta = { size: 0 };
        }

        this.meta = meta;
      }
    }

    return this.meta;
  }

  close() {
    this.fsync();

    this.openHandles = Math.max(this.openHandles - 1, 0);

    // Only close it if there are no existing open handles
    if (this.openHandles === 0) {
      this.ops.close();
    }
  }

  delete() {
    this.ops.delete();
  }

  load(indexes) {
    let status = indexes.reduce(
      (acc, b) => {
        let inMemory = this.buffer.get(b);
        if (inMemory) {
          acc.chunks.push(inMemory);
        } else {
          acc.missing.push(b);
        }
        return acc;
      },
      { chunks: [], missing: [] }
    );

    let missingChunks = [];
    if (status.missing.length > 0) {
      missingChunks = this.ops.readBlocks(status.missing, this.meta.blockSize);
    }
    return status.chunks.concat(missingChunks);
  }

  read(bufferView, offset, length, position) {
    // console.log('reading', this.filename, offset, length, position);
    let buffer = bufferView.buffer;

    if (length <= 0) {
      return 0;
    }
    if (position < 0) {
      // TODO: is this right?
      return 0;
    }
    if (position >= this.meta.size) {
      let view = new Uint8Array(buffer, offset);
      for (let i = 0; i < length; i++) {
        view[i] = 0;
      }

      return length;
    }

    position = Math.max(position, 0);
    let dataLength = Math.min(length, this.meta.size - position);

    let start = position;
    let end = position + dataLength;

    let indexes = getBoundaryIndexes(this.meta.blockSize, start, end);

    let chunks = this.load(indexes);
    let readBuffer = readChunks(chunks, start, end);

    if (buffer.byteLength - offset < readBuffer.byteLength) {
      throw new Error('Buffer given to `read` is too small');
    }
    let view = new Uint8Array(buffer);
    view.set(new Uint8Array(readBuffer), offset);

    // TODO: I don't need to do this. `unixRead` does this for us.
    for (let i = dataLength; i < length; i++) {
      view[offset + i] = 0;
    }

    return length;
  }

  write(bufferView, offset, length, position) {
    // console.log('writing', this.filename, offset, length, position);

    if (this.meta.blockSize == null) {
      // We don't have a block size yet (an empty file). The first
      // write MUST be the beginning of the file. This is a new file
      // and the first block contains the page size which we need.
      // sqlite will write this block first, and if you are directly
      // writing a db file to disk you can't write random parts of it.
      // Just write the whole thing and we'll get the first block
      // first.

      let pageSize = getPageSize(
        new Uint8Array(bufferView.buffer, bufferView.byteOffset + offset)
      );

      // Page sizes must be a power of 2 between 512 and 65536.
      // These was generated by doing `Math.pow(2, N)` where N >= 9
      // and N <= 16.
      if (
        ![512, 1024, 2048, 4096, 8192, 16384, 32768, 65536].includes(pageSize)
      ) {
        throw new Error(
          'File has invalid page size. (the first block of a new file must be written first)'
        );
      }

      this.setattr({ blockSize: pageSize });
    }

    let buffer = bufferView.buffer;

    if (length <= 0) {
      return 0;
    }
    if (position < 0) {
      return 0;
    }
    if (buffer.byteLength === 0) {
      return 0;
    }

    length = Math.min(length, buffer.byteLength - offset);

    let writes = writeChunks(
      new Uint8Array(buffer, offset, length),
      this.meta.blockSize,
      position,
      position + length
    );

    // Find any partial chunks and read them in and merge with
    // existing data
    let { partialWrites, fullWrites } = writes.reduce(
      (state, write) => {
        if (write.length !== this.meta.blockSize) {
          state.partialWrites.push(write);
        } else {
          state.fullWrites.push({
            pos: write.pos,
            data: write.data
          });
        }
        return state;
      },
      { fullWrites: [], partialWrites: [] }
    );

    let reads = [];
    if (partialWrites.length > 0) {
      reads = this.load(partialWrites.map(w => w.pos));
    }

    let allWrites = fullWrites.concat(
      reads.map(read => {
        let write = partialWrites.find(w => w.pos === read.pos);

        // MuTatIoN!
        new Uint8Array(read.data).set(
          new Uint8Array(write.data, write.offset, write.length),
          write.offset,
          write.length
        );

        return read;
      })
    );

    this.bufferChunks(allWrites);

    if (position + length > this.meta.size) {
      this.setattr({ size: position + length });
    }

    return length;
  }

  async readIfFallback() {
    if (this.ops.readIfFallback) {
      // Reset the meta
      let meta = await this.ops.readIfFallback();
      this.meta = meta || { size: 0 };
    }
  }

  lock(lockType) {
    // TODO: Perf APIs need improvement
    if (!this._recordingLock) {
      this._recordingLock = true;
    }

    if (this.ops.lock(lockType)) {
      if (lockType >= LOCK_TYPES.RESERVED) {
        this.writeLock = true;
      }
      return true;
    }
    return false;
  }

  unlock(lockType) {
    if (lockType === 0) {
      this._recordingLock = false;
    }

    if (this.writeLock) {
      // In certain cases (I saw this while running VACUUM after
      // changing page size) sqlite changes the size of the file
      // _after_ `fsync` for some reason. In our case, this is
      // critical because we are relying on fsync to write everything
      // out. If we just did some writes, do another fsync which will
      // check the meta and make sure it's persisted if dirty (all
      // other writes should already be flushed by now)
      this.fsync();
      this.writeLock = false;
    }

    return this.ops.unlock(lockType);
  }

  fsync() {
    if (this.buffer.size > 0) {
      // We need to handle page size changes which restructures the
      // whole db. We check if the page size is being written and
      // handle it
      let first = this.buffer.get(0);
      if (first) {
        let pageSize = getPageSize(new Uint8Array(first.data));

        if (pageSize !== this.meta.blockSize) {
          // The page size changed! We need to reflect that in our
          // storage. We need to restructure all pending writes and
          // change our page size so all future writes reflect the new
          // size.
          let buffer = this.buffer;
          this.buffer = new Map();

          // We take all pending writes, concat them into a single
          // buffer, and rewrite it out with the new size. This would
          // be dangerous if the page size could be changed at any
          // point in time since we don't handle partial reads here.
          // However sqlite only ever actually changes the page size
          // in 2 cases:
          //
          // * The db is empty (no data yet, so nothing to read)
          // * A VACUUM command is rewriting the entire db
          //
          // In both cases, we can assume we have _all_ the needed
          // data in the pending buffer, and we don't have to worry
          // about overwriting anything.

          let writes = [...buffer.values()];
          let totalSize = writes.length * this.meta.blockSize;
          let buf = new ArrayBuffer(totalSize);
          let view = new Uint8Array(buf);

          for (let write of writes) {
            view.set(new Uint8Array(write.data), write.pos);
          }

          // Rewrite the buffer with the new page size
          this.bufferChunks(writeChunks(view, pageSize, 0, totalSize));

          // Change our page size
          this.setattr({ blockSize: pageSize });
        }
      }

      this.ops.writeBlocks([...this.buffer.values()], this.meta.blockSize);
    }

    if (this._metaDirty) {
      // We only store the size right now. Block size is already
      // stored in the sqlite file and we don't need the rest
      //
      // TODO: Currently we don't delete any extra blocks after the
      // end of the file. This isn't super important, and in fact
      // could cause perf regressions (sqlite doesn't compress files
      // either!) but what we probably should do is detect a VACUUM
      // command (the whole db is being rewritten) and at that point
      // delete anything after the end of the file
      this.ops.writeMeta({ size: this.meta.size });
      this._metaDirty = false;
    }

    this.buffer = new Map();
  }

  setattr(attr) {
    if (this.meta == null) {
      this.meta = {};
    }

    // Size is the only attribute we actually persist. The rest are
    // stored in memory

    if (attr.mode !== undefined) {
      this.meta.mode = attr.mode;
    }

    if (attr.blockSize !== undefined) {
      this.meta.blockSize = attr.blockSize;
    }

    if (attr.size !== undefined) {
      this.meta.size = attr.size;
      this._metaDirty = true;
    }
  }

  getattr() {
    return this.meta;
  }
}

let FINALIZED = 0xdeadbeef;

let WRITEABLE = 0;
let READABLE = 1;

class Reader {
  constructor(
    buffer,
    { initialOffset = 4, useAtomics = true, stream = true, debug, name } = {}
  ) {
    this.buffer = buffer;
    this.atomicView = new Int32Array(buffer);
    this.offset = initialOffset;
    this.useAtomics = useAtomics;
    this.stream = stream;
    this.debug = debug;
    this.name = name;
  }

  log(...args) {
    if (this.debug) {
      console.log(`[reader: ${this.name}]`, ...args);
    }
  }

  waitWrite(name, timeout = null) {
    if (this.useAtomics) {
      this.log(`waiting for ${name}`);

      while (Atomics.load(this.atomicView, 0) === WRITEABLE) {
        if (timeout != null) {
          if (
            Atomics.wait(this.atomicView, 0, WRITEABLE, timeout) === 'timed-out'
          ) {
            throw new Error('timeout');
          }
        }

        Atomics.wait(this.atomicView, 0, WRITEABLE, 500);
      }

      this.log(`resumed for ${name}`);
    } else {
      if (this.atomicView[0] !== READABLE) {
        throw new Error('`waitWrite` expected array to be readable');
      }
    }
  }

  flip() {
    this.log('flip');
    if (this.useAtomics) {
      let prev = Atomics.compareExchange(
        this.atomicView,
        0,
        READABLE,
        WRITEABLE
      );

      if (prev !== READABLE) {
        throw new Error('Read data out of sync! This is disastrous');
      }

      Atomics.notify(this.atomicView, 0);
    } else {
      this.atomicView[0] = WRITEABLE;
    }

    this.offset = 4;
  }

  done() {
    this.waitWrite('done');

    let dataView = new DataView(this.buffer, this.offset);
    let done = dataView.getUint32(0) === FINALIZED;

    if (done) {
      this.log('done');
      this.flip();
    }

    return done;
  }

  peek(fn) {
    this.peekOffset = this.offset;
    let res = fn();
    this.offset = this.peekOffset;
    this.peekOffset = null;
    return res;
  }

  string(timeout) {
    this.waitWrite('string', timeout);

    let byteLength = this._int32();
    let length = byteLength / 2;

    let dataView = new DataView(this.buffer, this.offset, byteLength);
    let chars = [];
    for (let i = 0; i < length; i++) {
      chars.push(dataView.getUint16(i * 2));
    }
    let str = String.fromCharCode.apply(null, chars);
    this.log('string', str);

    this.offset += byteLength;

    if (this.peekOffset == null) {
      this.flip();
    }
    return str;
  }

  _int32() {
    let byteLength = 4;

    let dataView = new DataView(this.buffer, this.offset);
    let num = dataView.getInt32();
    this.log('_int32', num);

    this.offset += byteLength;
    return num;
  }

  int32() {
    this.waitWrite('int32');
    let num = this._int32();
    this.log('int32', num);

    if (this.peekOffset == null) {
      this.flip();
    }
    return num;
  }

  bytes() {
    this.waitWrite('bytes');

    let byteLength = this._int32();

    let bytes = new ArrayBuffer(byteLength);
    new Uint8Array(bytes).set(
      new Uint8Array(this.buffer, this.offset, byteLength)
    );
    this.log('bytes', bytes);

    this.offset += byteLength;

    if (this.peekOffset == null) {
      this.flip();
    }
    return bytes;
  }
}

class Writer {
  constructor(
    buffer,
    { initialOffset = 4, useAtomics = true, stream = true, debug, name } = {}
  ) {
    this.buffer = buffer;
    this.atomicView = new Int32Array(buffer);
    this.offset = initialOffset;
    this.useAtomics = useAtomics;
    this.stream = stream;

    this.debug = debug;
    this.name = name;

    if (this.useAtomics) {
      // The buffer starts out as writeable
      Atomics.store(this.atomicView, 0, WRITEABLE);
    } else {
      this.atomicView[0] = WRITEABLE;
    }
  }

  log(...args) {
    if (this.debug) {
      console.log(`[writer: ${this.name}]`, ...args);
    }
  }

  waitRead(name) {
    if (this.useAtomics) {
      this.log(`waiting for ${name}`);
      // Switch to writable
      // Atomics.store(this.atomicView, 0, 1);

      let prev = Atomics.compareExchange(
        this.atomicView,
        0,
        WRITEABLE,
        READABLE
      );

      if (prev !== WRITEABLE) {
        throw new Error(
          'Wrote something into unwritable buffer! This is disastrous'
        );
      }

      Atomics.notify(this.atomicView, 0);

      while (Atomics.load(this.atomicView, 0) === READABLE) {
        // console.log('waiting to be read...');
        Atomics.wait(this.atomicView, 0, READABLE, 500);
      }

      this.log(`resumed for ${name}`);
    } else {
      this.atomicView[0] = READABLE;
    }

    this.offset = 4;
  }

  finalize() {
    this.log('finalizing');
    let dataView = new DataView(this.buffer, this.offset);
    dataView.setUint32(0, FINALIZED);
    this.waitRead('finalize');
  }

  string(str) {
    this.log('string', str);

    let byteLength = str.length * 2;
    this._int32(byteLength);

    let dataView = new DataView(this.buffer, this.offset, byteLength);
    for (let i = 0; i < str.length; i++) {
      dataView.setUint16(i * 2, str.charCodeAt(i));
    }

    this.offset += byteLength;
    this.waitRead('string');
  }

  _int32(num) {
    let byteLength = 4;

    let dataView = new DataView(this.buffer, this.offset);
    dataView.setInt32(0, num);

    this.offset += byteLength;
  }

  int32(num) {
    this.log('int32', num);
    this._int32(num);
    this.waitRead('int32');
  }

  bytes(buffer) {
    this.log('bytes', buffer);

    let byteLength = buffer.byteLength;
    this._int32(byteLength);
    new Uint8Array(this.buffer, this.offset).set(new Uint8Array(buffer));

    this.offset += byteLength;
    this.waitRead('bytes');
  }
}

function positionToKey$1(pos, blockSize) {
  // We are forced to round because of floating point error. `pos`
  // should always be divisible by `blockSize`
  return Math.round(pos / blockSize);
}

function startWorker(reader, writer) {
  // In a normal world, we'd spawn the worker here as a child worker.
  // However Safari doesn't support nested workers, so we have to
  // proxy them through the main thread
  self.postMessage({
    type: '__absurd:spawn-idb-worker',
    argBuffer: writer.buffer,
    resultBuffer: reader.buffer
  });

  self.addEventListener('message', e => {
    switch (e.data.type) {
      // Normally you would use `postMessage` control the profiler in
      // a worker (just like this worker go those events), and the
      // perf library automatically handles those events. We don't do
      // that for the special backend worker though because it's
      // always blocked when it's not processing. Instead we forward
      // these events by going through the atomics layer to unblock it
      // to make sure it starts immediately
      case '__perf-deets:start-profile':
        writer.string('profile-start');
        writer.finalize();
        reader.int32();
        reader.done();
        break;

      case '__perf-deets:stop-profile':
        writer.string('profile-stop');
        writer.finalize();
        reader.int32();
        reader.done();
        break;
    }
  });
}

class FileOps {
  constructor(filename) {
    this.filename = filename;
  }

  // TODO: This should be renamed to `getDatabaseName`
  getStoreName() {
    return this.filename.replace(/\//g, '-');
  }

  invokeWorker(method, args) {
    if (this.reader == null || this.writer == null) {
      throw new Error(
        `Attempted ${method} on ${this.filename} but file not open`
      );
    }

    let reader = this.reader;
    let writer = this.writer;

    switch (method) {
      case 'readBlocks': {
        let { name, positions, blockSize } = args;

        let res = [];
        for (let pos of positions) {
          writer.string('readBlock');
          writer.string(name);
          writer.int32(positionToKey$1(pos, blockSize));
          writer.finalize();

          let data = reader.bytes();
          reader.done();
          res.push({
            pos,
            // If th length is 0, the block didn't exist. We return a
            // blank block in that case
            data: data.byteLength === 0 ? new ArrayBuffer(blockSize) : data
          });
        }

        return res;
      }

      case 'writeBlocks': {
        let { name, writes, blockSize } = args;
        writer.string('writeBlocks');
        writer.string(name);
        for (let write of writes) {
          writer.int32(positionToKey$1(write.pos, blockSize));
          writer.bytes(write.data);
        }
        writer.finalize();

        let res = reader.int32();
        reader.done();
        return res;
      }

      case 'readMeta': {
        writer.string('readMeta');
        writer.string(args.name);
        writer.finalize();

        let size = reader.int32();
        let blockSize = reader.int32();
        reader.done();
        return size === -1 ? null : { size, blockSize };
      }

      case 'writeMeta': {
        let { name, meta } = args;
        writer.string('writeMeta');
        writer.string(name);
        writer.int32(meta.size);
        // writer.int32(meta.blockSize);
        writer.finalize();

        let res = reader.int32();
        reader.done();
        return res;
      }

      case 'closeFile': {
        writer.string('closeFile');
        writer.string(args.name);
        writer.finalize();

        let res = reader.int32();
        reader.done();
        return res;
      }

      case 'lockFile': {
        writer.string('lockFile');
        writer.string(args.name);
        writer.int32(args.lockType);
        writer.finalize();

        let res = reader.int32();
        reader.done();
        return res === 0;
      }

      case 'unlockFile': {
        writer.string('unlockFile');
        writer.string(args.name);
        writer.int32(args.lockType);
        writer.finalize();

        let res = reader.int32();
        reader.done();
        return res === 0;
      }
    }
  }

  lock(lockType) {
    return this.invokeWorker('lockFile', {
      name: this.getStoreName(),
      lockType
    });
  }

  unlock(lockType) {
    return this.invokeWorker('unlockFile', {
      name: this.getStoreName(),
      lockType
    });
  }

  delete() {
    // Close the file if it's open
    if (this.reader || this.writer) {
      this.close();
    }

    // We delete it here because we can't do it in the worker; the
    // worker is stopped when the file closes. If we didn't do that,
    // workers would leak in the case of closing a file but not
    // deleting it. We could potentially restart the worker here if
    // needed, but for now just assume that the deletion is a success
    let req = globalThis.indexedDB.deleteDatabase(this.getStoreName());
    req.onerror = () => {
      console.warn(`Deleting ${this.filename} database failed`);
    };
    req.onsuccess = () => {};
  }

  open() {
    let argBuffer = new SharedArrayBuffer(4096 * 9);
    this.writer = new Writer(argBuffer, {
      name: 'args (backend)',
      debug: false
    });

    let resultBuffer = new SharedArrayBuffer(4096 * 9);
    this.reader = new Reader(resultBuffer, {
      name: 'results',
      debug: false
    });

    // TODO: We could pool workers and reuse them so opening files
    // aren't so slow
    startWorker(this.reader, this.writer);
  }

  close() {
    this.invokeWorker('closeFile', { name: this.getStoreName() });
    this.reader = null;
    this.writer = null;
    this.worker = null;
  }

  readMeta() {
    return this.invokeWorker('readMeta', { name: this.getStoreName() });
  }

  writeMeta(meta) {
    return this.invokeWorker('writeMeta', { name: this.getStoreName(), meta });
  }

  readBlocks(positions, blockSize) {
    if (this.stats) {
      this.stats.read += positions.length;
    }

    return this.invokeWorker('readBlocks', {
      name: this.getStoreName(),
      positions,
      blockSize
    });
  }

  writeBlocks(writes, blockSize) {
    if (this.stats) {
      this.stats.writes += writes.length;
    }

    return this.invokeWorker('writeBlocks', {
      name: this.getStoreName(),
      writes,
      blockSize
    });
  }
}

function positionToKey(pos, blockSize) {
  // We are forced to round because of floating point error. `pos`
  // should always be divisible by `blockSize`
  return Math.round(pos / blockSize);
}

async function openDb(name) {
  return new Promise((resolve, reject) => {
    let req = globalThis.indexedDB.open(name, 2);
    req.onsuccess = event => {
      let db = event.target.result;

      db.onversionchange = () => {
        console.log('closing because version changed');
        db.close();
      };
      db.onclose = () => {};

      resolve(db);
    };
    req.onupgradeneeded = event => {
      let db = event.target.result;
      if (!db.objectStoreNames.contains('data')) {
        db.createObjectStore('data');
      }
    };
    req.onblocked = e => console.log('blocked', e);
    req.onerror = req.onabort = e => reject(e.target.error);
  });
}

// Using a separate class makes it easier to follow the code, and
// importantly it removes any reliance on internal state in
// `FileOpsFallback`. That would be problematic since these method
// happen async; the args to `write` must be closed over so they don't
// change
class Persistance {
  constructor(onFallbackFailure) {
    this._openDb = null;
    this.hasAlertedFailure = false;
    this.onFallbackFailure = onFallbackFailure;
  }

  async getDb() {
    if (this._openDb) {
      return this._openDb;
    }

    this._openDb = await openDb(this.dbName);
    return this._openDb;
  }

  closeDb() {
    if (this._openDb) {
      this._openDb.close();
      this._openDb = null;
    }
  }

  // Both `readAll` and `write` rely on IndexedDB transactional
  // semantics to work, otherwise we'd have to coordinate them. If
  // there are pending writes, the `readonly` transaction in `readAll`
  // will block until they are all flushed out. If `write` is called
  // multiple times, `readwrite` transactions can only run one at a
  // time so it will naturally apply the writes sequentially (and
  // atomically)

  async readAll() {
    let db = await this.getDb(this.dbName);
    let blocks = new Map();

    let trans = db.transaction(['data'], 'readonly');
    let store = trans.objectStore('data');

    return new Promise((resolve, reject) => {
      // Open a cursor and iterate through the entire file
      let req = store.openCursor(IDBKeyRange.lowerBound(-1));
      req.onerror = reject;
      req.onsuccess = e => {
        let cursor = e.target.result;
        if (cursor) {
          blocks.set(cursor.key, cursor.value);
          cursor.continue();
        } else {
          resolve(blocks);
        }
      };
    });
  }

  async write(writes, cachedFirstBlock, hasLocked) {
    let db = await this.getDb(this.dbName);

    // We need grab a readwrite lock on the db, and then read to check
    // to make sure we can write to it
    let trans = db.transaction(['data'], 'readwrite');
    let store = trans.objectStore('data');

    await new Promise((resolve, reject) => {
      let req = store.get(0);
      req.onsuccess = e => {
        if (hasLocked) {
          if (!isSafeToWrite(req.result, cachedFirstBlock)) {
            if (this.onFallbackFailure && !this.hasAlertedFailure) {
              this.hasAlertedFailure = true;
              this.onFallbackFailure();
            }
            reject(new Error('Fallback mode unable to write file changes'));
            return;
          }
        }

        // Flush all the writes
        for (let write of writes) {
          store.put(write.value, write.key);
        }

        trans.onsuccess = () => resolve();
        trans.onerror = () => reject();
      };
      req.onerror = reject;
    });
  }
}

class FileOpsFallback {
  constructor(filename, onFallbackFailure) {
    this.filename = filename;
    this.dbName = this.filename.replace(/\//g, '-');
    this.cachedFirstBlock = null;
    this.writeQueue = null;
    this.blocks = new Map();
    this.lockType = 0;
    this.transferBlockOwnership = false;

    this.persistance = new Persistance(onFallbackFailure);
  }

  async readIfFallback() {
    this.transferBlockOwnership = true;
    this.blocks = await this.persistance.readAll();

    return this.readMeta();
  }

  lock(lockType) {
    // Locks always succeed here. Essentially we're only working
    // locally (we can't see any writes from anybody else) and we just
    // want to track the lock so we know when it downgrades from write
    // to read
    this.cachedFirstBlock = this.blocks.get(0);
    this.lockType = lockType;
    return true;
  }

  unlock(lockType) {
    if (this.lockType > LOCK_TYPES.SHARED && lockType === LOCK_TYPES.SHARED) {
      // Within a write lock, we delay all writes until the end of the
      // lock. We probably don't have to do this since we already
      // delay writes until an `fsync`, however this is an extra
      // measure to make sure we are writing everything atomically
      this.flush();
    }
    this.lockType = lockType;
    return true;
  }

  delete() {
    let req = globalThis.indexedDB.deleteDatabase(this.dbName);
    req.onerror = () => {
      console.warn(`Deleting ${this.filename} database failed`);
    };
    req.onsuccess = () => {};
  }

  open() {
    this.writeQueue = [];
    this.lockType = 0;
  }

  close() {
    this.flush();

    if (this.transferBlockOwnership) {
      this.transferBlockOwnership = false;
    } else {
      this.blocks = new Map();
    }

    this.persistance.closeDb();
  }

  readMeta() {
    let metaBlock = this.blocks.get(-1);
    if (metaBlock) {
      let block = this.blocks.get(0);

      return {
        size: metaBlock.size,
        blockSize: getPageSize(new Uint8Array(block))
      };
    }
    return null;
  }

  writeMeta(meta) {
    this.blocks.set(-1, meta);
    this.queueWrite(-1, meta);
  }

  readBlocks(positions, blockSize) {
    let res = [];
    for (let pos of positions) {
      res.push({
        pos,
        data: this.blocks.get(positionToKey(pos, blockSize))
      });
    }
    return res;
  }

  writeBlocks(writes, blockSize) {
    for (let write of writes) {
      let key = positionToKey(write.pos, blockSize);
      this.blocks.set(key, write.data);
      this.queueWrite(key, write.data);
    }

    // No write lock; flush them out immediately
    if (this.lockType <= LOCK_TYPES.SHARED) {
      this.flush();
    }
  }

  queueWrite(key, value) {
    this.writeQueue.push({ key, value });
  }

  flush() {
    if (this.writeQueue.length > 0) {
      this.persistance.write(
        this.writeQueue,
        this.cachedFirstBlock,
        this.lockType > LOCK_TYPES.SHARED
      );
      this.writeQueue = [];
    }
    this.cachedFirstBlock = null;
  }
}

class IndexedDBBackend {
  constructor(onFallbackFailure) {
    this.onFallbackFailure = onFallbackFailure;
  }

  createFile(filename) {
    let ops;
    if (typeof SharedArrayBuffer !== 'undefined') {
      // SharedArrayBuffer exists! We can run this fully
      ops = new FileOps(filename);
    } else {
      // SharedArrayBuffer is not supported. Use the fallback methods
      // which provide a somewhat working version, but doesn't
      // support mutations across connections (tabs)
      ops = new FileOpsFallback(filename, this.onFallbackFailure);
    }

    let file = new File(filename, ops);

    // If we don't need perf data, there's no reason for us to hold a
    // reference to the files. If we did we'd have to worry about
    // memory leaks
    if (process.env.NODE_ENV !== 'production' || process.env.PERF_BUILD) {
      if (this._files == null) {
        this._files = new Set();
      }
      this._files.add(file);
    }

    return file;
  }

  // Instead of controlling the profiler from the main thread by
  // posting a message to this worker, you can control it inside the
  // worker manually with these methods
  startProfile() {
    for (let file of this._files) {
      // If the writer doesn't exist, that means the file has been
      // deleted
      if (file.ops.writer) {
        let writer = file.ops.writer;
        let reader = file.ops.reader;
        writer.string('profile-start');
        writer.finalize();
        reader.int32();
        reader.done();
      }
    }
  }

  stopProfile() {
    for (let file of this._files) {
      if (file.ops.writer) {
        let writer = file.ops.writer;
        let reader = file.ops.reader;
        writer.string('profile-stop');
        writer.finalize();
        reader.int32();
        reader.done();
      }
    }
  }
}

const ERRNO_CODES = {
  EPERM: 63,
  ENOENT: 44
};

// This implements an emscripten-compatible filesystem that is means
// to be mounted to the one from `sql.js`. Example:
//
// let BFS = new SQLiteFS(SQL.FS, idbBackend);
// SQL.FS.mount(BFS, {}, '/blocked');
//
// Now any files created under '/blocked' will be handled by this
// filesystem, which creates a special file that handles read/writes
// in the way that we want.
class SQLiteFS$1 {
  constructor(FS, backend) {
    this.FS = FS;
    this.backend = backend;

    this.node_ops = {
      getattr: node => {
        let fileattr = FS.isFile(node.mode) ? node.contents.getattr() : null;

        let attr = {};
        attr.dev = 1;
        attr.ino = node.id;
        attr.mode = fileattr ? fileattr.mode : node.mode;
        attr.nlink = 1;
        attr.uid = 0;
        attr.gid = 0;
        attr.rdev = node.rdev;
        attr.size = fileattr ? fileattr.size : FS.isDir(node.mode) ? 4096 : 0;
        attr.atime = new Date(0);
        attr.mtime = new Date(0);
        attr.ctime = new Date(0);
        attr.blksize = fileattr ? fileattr.blockSize : 4096;
        attr.blocks = Math.ceil(attr.size / attr.blksize);
        return attr;
      },
      setattr: (node, attr) => {
        if (this.FS.isFile(node.mode)) {
          node.contents.setattr(attr);
        } else {
          if (attr.mode != null) {
            node.mode = attr.mode;
          }
          if (attr.size != null) {
            node.size = attr.size;
          }
        }
      },
      lookup: (parent, name) => {
        throw new this.FS.ErrnoError(ERRNO_CODES.ENOENT);
      },
      mknod: (parent, name, mode, dev) => {
        if (name.endsWith('.lock')) {
          throw new Error('Locking via lockfiles is not supported');
        }

        return this.createNode(parent, name, mode, dev);
      },
      rename: (old_node, new_dir, new_name) => {
        throw new Error('rename not implemented');
      },
      unlink: (parent, name) => {
        let node = this.FS.lookupNode(parent, name);
        node.contents.delete(name);
      },
      readdir: node => {
        // We could list all the available databases here if `node` is
        // the root directory. However Firefox does not implemented
        // such a methods. Other browsers do, but since it's not
        // supported on all browsers users will need to track it
        // separate anyway right now

        throw new Error('readdir not implemented');
      },
      symlink: (parent, newname, oldpath) => {
        throw new Error('symlink not implemented');
      },
      readlink: node => {
        throw new Error('symlink not implemented');
      }
    };

    this.stream_ops = {
      open: stream => {
        if (this.FS.isFile(stream.node.mode)) {
          stream.node.contents.open();
        }
      },

      close: stream => {
        if (this.FS.isFile(stream.node.mode)) {
          stream.node.contents.close();
        }
      },

      read: (stream, buffer, offset, length, position) => {
        // console.log('read', offset, length, position)
        return stream.node.contents.read(buffer, offset, length, position);
      },

      write: (stream, buffer, offset, length, position) => {
        // console.log('write', offset, length, position);
        return stream.node.contents.write(buffer, offset, length, position);
      },

      llseek: (stream, offset, whence) => {
        // Copied from MEMFS
        var position = offset;
        if (whence === 1) {
          position += stream.position;
        } else if (whence === 2) {
          if (FS.isFile(stream.node.mode)) {
            position += stream.node.contents.getattr().size;
          }
        }
        if (position < 0) {
          throw new this.FS.ErrnoError(28);
        }
        return position;
      },
      allocate: (stream, offset, length) => {
        stream.node.contents.setattr({ size: offset + length });
      },
      mmap: (stream, address, length, position, prot, flags) => {
        throw new Error('mmap not implemented');
      },
      msync: (stream, buffer, offset, length, mmapFlags) => {
        throw new Error('msync not implemented');
      },
      fsync: (stream, buffer, offset, length, mmapFlags) => {
        stream.node.contents.fsync();
      }
    };
  }

  mount() {
    return this.createNode(null, '/', 16384 /* dir */ | 511 /* 0777 */, 0);
  }

  lock(path, lockType) {
    let { node } = this.FS.lookupPath(path);
    return node.contents.lock(lockType);
  }

  unlock(path, lockType) {
    let { node } = this.FS.lookupPath(path);
    return node.contents.unlock(lockType);
  }

  createNode(parent, name, mode, dev) {
    // Only files and directories supported
    if (!(this.FS.isDir(mode) || this.FS.isFile(mode))) {
      throw new this.FS.ErrnoError(ERRNO_CODES.EPERM);
    }

    var node = this.FS.createNode(parent, name, mode, dev);
    if (this.FS.isDir(node.mode)) {
      node.node_ops = {
        mknod: this.node_ops.mknod,
        lookup: this.node_ops.lookup,
        unlink: this.node_ops.unlink,
        setattr: this.node_ops.setattr
      };
      node.stream_ops = {};
      node.contents = {};
    } else if (this.FS.isFile(node.mode)) {
      node.node_ops = this.node_ops;
      node.stream_ops = this.stream_ops;

      // Create file!
      node.contents = this.backend.createFile(name);
    }

    // add the new node to the parent
    if (parent) {
      parent.contents[name] = node;
      parent.timestamp = node.timestamp;
    }

    return node;
  }
}

// Right now we don't support `export from` so we do this manually
const SQLiteFS = SQLiteFS$1;

class WebDao {
  _db;
  _path = "/sql/db.sqlite";
  _cacheSize = 5000;
  _pageSize = 8192;
  _SQL = null;
  _sqlFS = null;

  constructor() {}

  async init(initSqlJs) {
    this._SQL = await initSqlJs({ locateFile: (file) => file });
    this._sqlFS = new SQLiteFS(this._SQL.FS, new IndexedDBBackend());
    this._SQL.register_for_idb(this._sqlFS);

    this._SQL.FS.mkdir("/sql");
    this._SQL.FS.mount(this._sqlFS, {}, "/sql");
  }
  async open() {
    if (typeof SharedArrayBuffer === "undefined") {
      let stream = this._SQL.FS.open(this._path, "a+");
      await stream.node.contents.readIfFallback();
      this._SQL.FS.close(stream);
    }

    this._db = new this._SQL.Database(this._path, { filename: true });
    this._db.exec(`
        PRAGMA cache_size=-${this._cacheSize};
        PRAGMA page_size=${this._pageSize};
        PRAGMA journal_mode=MEMORY;
      `);
    console.log("Created db");
    this._db.exec("VACUUM");
  }

  close() {
    if (this._db) {
      console.log("MUST HAVE BEEN ERROR! CLOSING DATABASE TO REPOEN!");
      this._db.close();
      this._db = null;
    }
  }

  prepare(sql) {
    return this._db.prepare(sql);
  }
  async runPrepare(prep, args = [], fn) {
    try {
      prep.run([...args]);
    } catch (error) {
      this.close();
      await this.open();
      prep.run([...args]);
      return fn && fn();
    }
    return fn && fn();
  }

  async handleLock() {
    this.close();
    await this.open();
  }
  async exec(sql) {
    try {
      this._db.exec(sql);
    } catch (error) {
      if (sql !== "COMMIT") {
        await this.handleLock();
        this._db.exec(sql);
      }
    }
  }
  async run(sql) {
    try {
      this._db.run(sql);
    } catch (error) {
      if (sql !== "COMMIT") {
        await this.handleLock();
        this._db.run(sql);
      }
    }
  }

  queryAll(sql, params = {}) {
    let q = this.buildPreparedStatement(sql, Object.keys(params));
    let stmt = this.prepare(q);
    stmt.bind({ ...params });
    let rows = [];
    while (stmt.step()) {
      rows = [...rows, stmt.getAsObject()];
    }
    console.log("Here is a queryAll rows: " + JSON.stringify(rows));
    return rows;
  }
  queryObj(sql, params = {}) {
    let returnObject;
    const s = this.buildPreparedStatement(sql, Object.keys(params));
    let stmt = this._db.prepare(s);
    returnObject = stmt.getAsObject(params);

    return returnObject;
  }

  buildPreparedStatement(query, fields = []) {
    if (!fields || !fields.length) {
      return query;
    }
    /** Query starts as select * from something WHERE ? */
    const nq = fields.reduce((acc, curr) => {
      let accQuery = acc.replace("?", curr);
      return accQuery;
    }, query);
    return nq;
  }
}

const startDatabase = async () => {
  getWorker();
  return new Promise((res, rej) => {
    window.worker.postMessage({ type: "INIT_DATABASE" });
    window.worker.addEventListener("message", (e) => {
      if (e.data.type === "INITIALIZED_DB") {
        return res(true);
      }
    });
  });
};
const startClock = async () => {
  const c = await makeClock(new Timestamp(0, 0, makeClientId(true)));
  return setClock(c);
};

const setEnvironment = (e) => {
  EnvironmentState.set(e);
};
EnvironmentState.subscribe((env) => (env));

let syncInterval;
function startSync() {
  syncInterval = setInterval(async () => {
    try {
      await sync();
      EnvironmentState.update((env) => Object.assign(env, { isOffline: false }));
    } catch (e) {
      if (e.message === "network-failure") {
        EnvironmentState.update((env) => Object.assign(env, { isOffline: true }));
      } else {
        throw e;
      }
    }
    // TODO: Make this configurable
  }, 4000);
}

function stopSync() {
  clearInterval(syncInterval);
}

function select(sql) {
  return new Promise((res, rej) => {
    window.worker.postMessage({ type: "SELECT_ALL", sql });
    window.worker.addEventListener("message", function (e) {
      if (e.data.type === "SELECT") {
        return res(e.data.results);
      }
    });
  });
}

function decodeBase64(base64, enableUnicode) {
    var binaryString = atob(base64);
    if (enableUnicode) {
        var binaryView = new Uint8Array(binaryString.length);
        for (var i = 0, n = binaryString.length; i < n; ++i) {
            binaryView[i] = binaryString.charCodeAt(i);
        }
        return String.fromCharCode.apply(null, new Uint16Array(binaryView.buffer));
    }
    return binaryString;
}

function createURL(base64, sourcemapArg, enableUnicodeArg) {
    var sourcemap = sourcemapArg === undefined ? null : sourcemapArg;
    var enableUnicode = enableUnicodeArg === undefined ? false : enableUnicodeArg;
    var source = decodeBase64(base64, enableUnicode);
    var start = source.indexOf('\n', 10) + 1;
    var body = source.substring(start) + (sourcemap ? '\/\/# sourceMappingURL=' + sourcemap : '');
    var blob = new Blob([body], { type: 'application/javascript' });
    return URL.createObjectURL(blob);
}

function createBase64WorkerFactory(base64, sourcemapArg, enableUnicodeArg) {
    var url;
    return function WorkerFactory(options) {
        url = url || createURL(base64, sourcemapArg, enableUnicodeArg);
        return new Worker(url, options);
    };
}

var WorkerFactory = createBase64WorkerFactory('/* rollup-plugin-web-worker-loader */
!function(){"use strict";let t=3735928559;class e{constructor(t,{initialOffset:e=4,useAtomics:i=!0,stream:s=!0,debug:r,name:n}={}){this.buffer=t,this.atomicView=new Int32Array(t),this.offset=e,this.useAtomics=i,this.stream=s,this.debug=r,this.name=n}log(...t){this.debug&&console.log(`[reader: ${this.name}]`,...t)}waitWrite(t,e=null){if(this.useAtomics){for(this.log(`waiting for ${t}`);0===Atomics.load(this.atomicView,0);){if(null!=e&&"timed-out"===Atomics.wait(this.atomicView,0,0,e))throw new Error("timeout");Atomics.wait(this.atomicView,0,0,500)}this.log(`resumed for ${t}`)}else if(1!==this.atomicView[0])throw new Error("`waitWrite` expected array to be readable")}flip(){if(this.log("flip"),this.useAtomics){if(1!==Atomics.compareExchange(this.atomicView,0,1,0))throw new Error("Read data out of sync! This is disastrous");Atomics.notify(this.atomicView,0)}else this.atomicView[0]=0;this.offset=4}done(){this.waitWrite("done");let e=new DataView(this.buffer,this.offset).getUint32(0)===t;return e&&(this.log("done"),this.flip()),e}peek(t){this.peekOffset=this.offset;let e=t();return this.offset=this.peekOffset,this.peekOffset=null,e}string(t){this.waitWrite("string",t);let e=this._int32(),i=e/2,s=new DataView(this.buffer,this.offset,e),r=[];for(let t=0;t<i;t++)r.push(s.getUint16(2*t));let n=String.fromCharCode.apply(null,r);return this.log("string",n),this.offset+=e,null==this.peekOffset&&this.flip(),n}_int32(){let t=new DataView(this.buffer,this.offset).getInt32();return this.log("_int32",t),this.offset+=4,t}int32(){this.waitWrite("int32");let t=this._int32();return this.log("int32",t),null==this.peekOffset&&this.flip(),t}bytes(){this.waitWrite("bytes");let t=this._int32(),e=new ArrayBuffer(t);return new Uint8Array(e).set(new Uint8Array(this.buffer,this.offset,t)),this.log("bytes",e),this.offset+=t,null==this.peekOffset&&this.flip(),e}}class i{constructor(t,{initialOffset:e=4,useAtomics:i=!0,stream:s=!0,debug:r,name:n}={}){this.buffer=t,this.atomicView=new Int32Array(t),this.offset=e,this.useAtomics=i,this.stream=s,this.debug=r,this.name=n,this.useAtomics?Atomics.store(this.atomicView,0,0):this.atomicView[0]=0}log(...t){this.debug&&console.log(`[writer: ${this.name}]`,...t)}waitRead(t){if(this.useAtomics){if(this.log(`waiting for ${t}`),0!==Atomics.compareExchange(this.atomicView,0,0,1))throw new Error("Wrote something into unwritable buffer! This is disastrous");for(Atomics.notify(this.atomicView,0);1===Atomics.load(this.atomicView,0);)Atomics.wait(this.atomicView,0,1,500);this.log(`resumed for ${t}`)}else this.atomicView[0]=1;this.offset=4}finalize(){this.log("finalizing"),new DataView(this.buffer,this.offset).setUint32(0,t),this.waitRead("finalize")}string(t){this.log("string",t);let e=2*t.length;this._int32(e);let i=new DataView(this.buffer,this.offset,e);for(let e=0;e<t.length;e++)i.setUint16(2*e,t.charCodeAt(e));this.offset+=e,this.waitRead("string")}_int32(t){new DataView(this.buffer,this.offset).setInt32(0,t),this.offset+=4}int32(t){this.log("int32",t),this._int32(t),this.waitRead("int32")}bytes(t){this.log("bytes",t);let e=t.byteLength;this._int32(e),new Uint8Array(this.buffer,this.offset).set(new Uint8Array(t)),this.offset+=e,this.waitRead("bytes")}}let s=0,r=1,n=2,o=4;let a=/^((?!chrome|android).)*safari/i.test(navigator.userAgent),l=new Map,c=new Map;function h(t,e){if(!t)throw new Error(e)}class f{constructor(t,e="readonly"){this.db=t,this.trans=this.db.transaction(["data"],e),this.store=this.trans.objectStore("data"),this.lockType="readonly"===e?r:o,this.cachedFirstBlock=null,this.cursor=null,this.prevReads=null}async prefetchFirstBlock(t){let e=await this.get(0);return this.cachedFirstBlock=e,e}async waitComplete(){return new Promise(((t,e)=>{this.commit(),this.lockType===o?(this.trans.oncomplete=e=>t(),this.trans.onerror=t=>e(t)):a?this.trans.oncomplete=e=>t():t()}))}commit(){this.trans.commit&&this.trans.commit()}async upgradeExclusive(){this.commit(),this.trans=this.db.transaction(["data"],"readwrite"),this.store=this.trans.objectStore("data"),this.lockType=o;let t=this.cachedFirstBlock;return function(t,e){if(null!=t&&null!=e){let i=new Uint8Array(t),s=new Uint8Array(e);for(let t=24;t<40;t++)if(i[t]!==s[t])return!1;return!0}return null==t&&null==e}(await this.prefetchFirstBlock(500),t)}downgradeShared(){this.commit(),this.trans=this.db.transaction(["data"],"readonly"),this.store=this.trans.objectStore("data"),this.lockType=r}async get(t){return new Promise(((e,i)=>{let s=this.store.get(t);s.onsuccess=t=>{e(s.result)},s.onerror=t=>i(t)}))}getReadDirection(){let t=this.prevReads;if(t){if(t[0]<t[1]&&t[1]<t[2]&&t[2]-t[0]<10)return"next";if(t[0]>t[1]&&t[1]>t[2]&&t[0]-t[2]<10)return"prev"}return null}read(t){let e=()=>new Promise(((t,e)=>{if(null!=this.cursorPromise)throw new Error("waitCursor() called but something else is already waiting");this.cursorPromise={resolve:t,reject:e}}));if(this.cursor){let i=this.cursor;return"next"===i.direction&&t>i.key&&t<i.key+100?(i.advance(t-i.key),e()):"prev"===i.direction&&t<i.key&&t>i.key-100?(i.advance(i.key-t),e()):(this.cursor=null,this.read(t))}{let i=this.getReadDirection();if(i){let s;this.prevReads=null,s="prev"===i?IDBKeyRange.upperBound(t):IDBKeyRange.lowerBound(t);let r=this.store.openCursor(s,i);return r.onsuccess=t=>{let e=t.target.result;if(this.cursor=e,null==this.cursorPromise)throw new Error("Got data from cursor but nothing is waiting it");this.cursorPromise.resolve(e?e.value:null),this.cursorPromise=null},r.onerror=t=>{if(console.log("Cursor failure:",t),null==this.cursorPromise)throw new Error("Got data from cursor but nothing is waiting it");this.cursorPromise.reject(t),this.cursorPromise=null},e()}return null==this.prevReads&&(this.prevReads=[0,0,0]),this.prevReads.push(t),this.prevReads.shift(),this.get(t)}}async set(t){return this.prevReads=null,new Promise(((e,i)=>{let s=this.store.put(t.value,t.key);s.onsuccess=t=>e(s.result),s.onerror=t=>i(t)}))}async bulkSet(t){this.prevReads=null;for(let e of t)this.store.put(e.value,e.key)}}async function u(t){return new Promise(((e,i)=>{if(l.get(t))return void e(l.get(t));let s=globalThis.indexedDB.open(t,2);s.onsuccess=i=>{let s=i.target.result;s.onversionchange=()=>{console.log("closing because version changed"),s.close(),l.delete(t)},s.onclose=()=>{l.delete(t)},l.set(t,s),e(s)},s.onupgradeneeded=t=>{let e=t.target.result;e.objectStoreNames.contains("data")||e.createObjectStore("data")},s.onblocked=t=>console.log("blocked",t),s.onerror=s.onabort=t=>i(t.target.error)}))}async function w(t,e,i){let s=c.get(t);if(s){if("readwrite"===e&&s.lockType===r)throw new Error("Attempted write but only has SHARED lock");return i(s)}s=new f(await u(t),e),await i(s),await s.waitComplete()}async function d(t,e,i){let n=function(t){return c.get(t)}(e);if(i===r){if(null==n)throw new Error("Unlock error (SHARED): no transaction running");n.lockType===o&&n.downgradeShared()}else i===s&&n&&(await n.waitComplete(),c.delete(e));t.int32(0),t.finalize()}async function g(t,e){let i=t.string();switch(i){case"profile-start":t.done(),e.int32(0),e.finalize(),g(t,e);break;case"profile-stop":t.done(),await new Promise((t=>setTimeout(t,1e3))),e.int32(0),e.finalize(),g(t,e);break;case"writeBlocks":{let i=t.string(),s=[];for(;!t.done();){let e=t.int32(),i=t.bytes();s.push({pos:e,data:i})}await async function(t,e,i){return w(e,"readwrite",(async e=>{await e.bulkSet(i.map((t=>({key:t.pos,value:t.data})))),t.int32(0),t.finalize()}))}(e,i,s),g(t,e);break}case"readBlock":{let i=t.string(),s=t.int32();t.done(),await async function(t,e,i){return w(e,"readonly",(async e=>{let s=await e.read(i);null==s?t.bytes(new ArrayBuffer(0)):t.bytes(s),t.finalize()}))}(e,i,s),g(t,e);break}case"readMeta":{let i=t.string();t.done(),await async function(t,e){return w(e,"readonly",(async i=>{try{console.log("Reading meta...");let s=await i.get(-1);if(console.log(`Got meta for ${e}:`,s),null==s)t.int32(-1),t.int32(4096),t.finalize();else{let e=await i.get(0),r=4096;e&&(r=256*new Uint16Array(e)[8]),t.int32(s.size),t.int32(r),t.finalize()}}catch(e){console.log(e),t.int32(-1),t.int32(-1),t.finalize()}}))}(e,i),g(t,e);break}case"writeMeta":{let i=t.string(),s=t.int32();t.done(),await async function(t,e,i){return w(e,"readwrite",(async e=>{try{await e.set({key:-1,value:i}),t.int32(0),t.finalize()}catch(e){console.log(e),t.int32(-1),t.finalize()}}))}(e,i,{size:s}),g(t,e);break}case"closeFile":{let i=t.string();t.done(),e.int32(0),e.finalize(),function(t){let e=l.get(t);e&&(e.close(),l.delete(t))}(i),self.close();break}case"lockFile":{let i=t.string(),s=t.int32();t.done(),await async function(t,e,i){let s=c.get(e);if(s)if(i>s.lockType){h(s.lockType===r,`Uprading lock type from ${s.lockType} is invalid`),h(i===n||i===o,`Upgrading lock type to ${i} is invalid`);let e=await s.upgradeExclusive();t.int32(e?0:-1),t.finalize()}else h(s.lockType===i,`Downgrading lock to ${i} is invalid`),t.int32(0),t.finalize();else{h(i===r,`New locks must start as SHARED instead of ${i}`);let s=new f(await u(e));await s.prefetchFirstBlock(500),c.set(e,s),t.int32(0),t.finalize()}}(e,i,s),g(t,e);break}case"unlockFile":{let i=t.string(),s=t.int32();t.done(),await d(e,i,s),g(t,e);break}default:throw new Error("Unknown method: "+i)}}self.onmessage=t=>{switch(t.data.type){case"init":{let[s,r]=t.data.buffers;g(new e(s,{name:"args",debug:!1}),new i(r,{name:"results",debug:!1}));break}}}}();

', null, false);

var indexeddbMainThreadWorkerE59fee74 = /*#__PURE__*/Object.freeze({
  __proto__: null,
  'default': WorkerFactory
});

export { datastore as DatastoreState, EnvironmentState as Environment, MeadoCrypto, messageBus as MessageBus, MutableTimestamp, Timestamp, WebDao, _delete, apply, bootstrap, buildSchema, debounce, deserializeValue, getClock, getWorker, insert, makeClientId, merkle, registerApply, select, serializeValue, setEnvironment, startClock, startDatabase, startSync, stopSync, sync, update, writable };
